{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO # yolov8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'model\\yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 19.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('model/yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deque = {}\n",
    "deep_sort = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color_for_labels(label):\n",
    "    \"\"\"\n",
    "    Compute a color for a given label.\n",
    "    :param label: The label for which to compute the color.\n",
    "    :return: The computed color as a tuple of RGB values.\n",
    "    \"\"\"\n",
    "    color = label % 16_777_216  # Limit the label to the range of 0 to 16,777,215\n",
    "    r = (color // 65536) % 256\n",
    "    g = (color // 256) % 256\n",
    "    b = color % 256\n",
    "    return r, g, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_border(img, pt1, pt2, color, thickness, r, d):\n",
    "    '''\n",
    "    Draw a fancy border around the given image\n",
    "    :param img: The image to draw the border on.\n",
    "    :param pt1: Top left point.\n",
    "    :param pt2: Bottom right point.\n",
    "    :param color: Color of the border.\n",
    "    :param thickness: Thickness of the border.\n",
    "    :param r: Border radius.\n",
    "    :param d: Border line length.\n",
    "    :return: The image with the drawn border.\n",
    "    '''\n",
    "    \n",
    "    x1,y1 = pt1\n",
    "    x2,y2 = pt2\n",
    "    # Top left\n",
    "    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n",
    "    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n",
    "    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n",
    "    # Top right\n",
    "    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n",
    "    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n",
    "    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n",
    "    # Bottom left\n",
    "    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n",
    "    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n",
    "    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n",
    "    # Bottom right\n",
    "    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n",
    "    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n",
    "    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)\n",
    "\n",
    "    cv2.rectangle(img, (x1 + r, y1), (x2 - r, y2), color, -1, cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x1, y1 + r), (x2, y2 - r - d), color, -1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.circle(img, (x1 +r, y1+r), 2, color, 12)\n",
    "    cv2.circle(img, (x2 -r, y1+r), 2, color, 12)\n",
    "    cv2.circle(img, (x1 +r, y2-r), 2, color, 12)\n",
    "    cv2.circle(img, (x2 -r, y2-r), 2, color, 12)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UI_box(x, img, color = None, label = None, line_thickness = None):\n",
    "    \"\"\"\n",
    "    Draws a UI box on the image\n",
    "    :param x: The coordinates of the box.\n",
    "    :param img: The image to draw the box on.\n",
    "    :param color: The color of the box.\n",
    "    :param label: The label of the box.\n",
    "    :param line_thickness: The thickness of the box.\n",
    "    :return: The image with the drawn box.\n",
    "    \"\"\"\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale = tl / 3, thickness = tf)[0]\n",
    "\n",
    "        img = draw_border(img, (c1[0], c1[1] - t_size[1] - 3), (c1[0] + t_size[0], c1[1] + 3), color, 1, 8, 2)\n",
    "\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness = tf, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tracker(): # Apply DEEPSORT\n",
    "    \"\"\"\n",
    "    Initialize the deep sort tracker\n",
    "    config file is located at deep_sort/configs/deep_sort.yaml\n",
    "    \"\"\"\n",
    "    global deep_sort\n",
    "    cfg_deep = get_config()\n",
    "    cfg_deep.merge_from_file(\"deep_sort/configs/deep_sort.yaml\")\n",
    "\n",
    "    deep_sort= DeepSort(cfg_deep.DEEPSORT.REID_CKPT,\n",
    "                            max_dist = cfg_deep.DEEPSORT.MAX_DIST, min_confidence = cfg_deep.DEEPSORT.MIN_CONFIDENCE,\n",
    "                            nms_max_overlap = cfg_deep.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance = cfg_deep.DEEPSORT.MAX_IOU_DISTANCE,\n",
    "                            max_age = cfg_deep.DEEPSORT.MAX_AGE, n_init = cfg_deep.DEEPSORT.N_INIT, nn_budget = cfg_deep.DEEPSORT.NN_BUDGET,\n",
    "                            use_cuda = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(img, bbox, names, object_id, identities=None, offset=(0, 0)):\n",
    "    \"\"\"\n",
    "    Draws the bounding boxes on the image.\n",
    "    :param img: The image to draw the bounding boxes on.\n",
    "    :param bbox: The bounding boxes.\n",
    "    :param names: The names of the objects.\n",
    "    :param identities: The identities of the objects.\n",
    "    :param offset: The offset of the bounding boxes.\n",
    "    :return: The image with the drawn bounding boxes.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "    \n",
    "        # code to find center of bottom edge\n",
    "        center = (int((x2+x1) / 2), int((y2+y2) / 2))\n",
    "\n",
    "        # get ID of object\n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "\n",
    "        # create new buffer for new object\n",
    "        if id not in data_deque:  \n",
    "          data_deque[id] = deque(maxlen = 64)\n",
    "        if i < len(object_id):\n",
    "            color = compute_color_for_labels(object_id[i])\n",
    "            obj_name = names[object_id[i]]\n",
    "            label = '{}{:d}'.format(\"\", id) + \":\"+ '%s' % (obj_name)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        UI_box(box, img, label = label, color = color, line_thickness = 2)\n",
    "        \n",
    "        # draw center of bottom edge\n",
    "        cv2.circle(img, center, 2, color, 12)\n",
    "        data_deque[id].append(center)\n",
    "        for j in range(1, len(data_deque[id])):\n",
    "            if data_deque[id][j - 1] is None or data_deque[id][j] is None:\n",
    "                continue\n",
    "            thickness = int(np.sqrt(64 / float(j + 1)) * 2)\n",
    "            cv2.line(img, (data_deque[id][j - 1]), (data_deque[id][j]), color, thickness)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs, time, patience, batch, imgsz, save = True, device = None, worker = -1, project = None, name = None, pretrained = True, verbose = False, seed = 0, optimizer = 'auto'):\n",
    "    \"\"\"\n",
    "    Trains the model with the given data.\n",
    "    :param model: The model to train.\n",
    "    :param data: The data to train the model with.\n",
    "    :param epochs: The number of epochs to train the model.\n",
    "    :param time: The time to train the model.\n",
    "    :param patience: The patience for the early stopping.\n",
    "    :param batch: The batch size for the training.\n",
    "    :param imgsz: The size of the images.\n",
    "    :param save: Whether to save the model.\n",
    "    :param device: The device to train the model on.\n",
    "    :param worker: The number of workers for the training.\n",
    "    :param project: The project name for the training.\n",
    "    :param name: The name of the model for the training.\n",
    "    :param pretrained: Whether to use a pretrained model.\n",
    "    :param verbose: Whether to print the training information.\n",
    "    :param seed: The seed for the training.\n",
    "    :param optimizer: The optimizer for the training.\n",
    "    :return: The trained model.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if worker == -1:\n",
    "        worker = min([os.cpu_count(), 8])\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    if pretrained:\n",
    "        model.half()\n",
    "    model.to(device).train()\n",
    "    dataset = data\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch, num_workers = worker, shuffle = True, pin_memory = True, collate_fn = dataset.collate_fn)\n",
    "    optimizer = model.configure_optimizers(optimizer)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [round(0.5 * epochs), round(0.75 * epochs)], gamma = 0.1)\n",
    "    start_time = time.time()\n",
    "    best_fitness = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.fitness = 0\n",
    "        model.loss = 0\n",
    "        model.metrics = []\n",
    "        pbar = tqdm(dataloader, desc = f'Epoch {epoch + 1}/{epochs}', unit = 'batch')\n",
    "        for i, (imgs, targets, paths, _) in enumerate(pbar):\n",
    "            imgs = imgs.to(device, non_blocking=True).half() if torch.cuda.is_available() else imgs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device)\n",
    "            model.zero_grad()\n",
    "            loss, outputs = model(imgs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.loss += loss.item()\n",
    "            model.fitness += outputs[0].mean().item()\n",
    "            model.metrics.append(outputs)\n",
    "            pbar.set_postfix(loss=model.loss / (i + 1), fitness=model.fitness / (i + 1))\n",
    "        model.loss /= len(dataloader)\n",
    "        model.fitness /= len(dataloader)\n",
    "        scheduler.step()\n",
    "        if model.fitness > best_fitness:\n",
    "            best_fitness = model.fitness\n",
    "            if save:\n",
    "                model.save(name, epoch, project)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Fitness: {model.fitness:.6f}, Loss: {model.loss:.6f}')\n",
    "        if time.time() - start_time > time:\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(data, device = None, worker = -1, verbose = False, seed = 0):\n",
    "    \"\"\"\n",
    "    Validates the model with the given data.\n",
    "    :param model: The model to validate.\n",
    "    :param data: The data to validate the model with.\n",
    "    :param device: The device to validate the model on.\n",
    "    :param worker: The number of workers for the validation.\n",
    "    :param verbose: Whether to print the validation information.\n",
    "    :param seed: The seed for the validation.\n",
    "    :return: The validation results.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if worker == -1:\n",
    "        worker = min([os.cpu_count(), 8])\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    model.to(device).eval()\n",
    "    dataset = data\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1, num_workers = worker, shuffle = False, pin_memory = True, collate_fn = dataset.collate_fn)\n",
    "    model.fitness = 0\n",
    "    model.loss = 0\n",
    "    model.metrics = []\n",
    "    pbar = tqdm(dataloader, desc = 'Validation', unit = 'batch')\n",
    "    for i, (imgs, targets, paths, _) in enumerate(pbar):\n",
    "        imgs = imgs.to(device, non_blocking = True).half() if torch.cuda.is_available() else imgs.to(device, non_blocking = True)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss, outputs = model(imgs, targets)\n",
    "        model.loss += loss.item()\n",
    "        model.fitness += outputs[0].mean().item()\n",
    "        model.metrics.append(outputs)\n",
    "        pbar.set_postfix(loss = model.loss / (i + 1), fitness=model.fitness / (i + 1))\n",
    "    model.loss /= len(dataloader)\n",
    "    model.fitness /= len(dataloader)\n",
    "    if verbose:\n",
    "        print(f'Fitness: {model.fitness:.6f}, Loss: {model.loss:.6f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_test = None, vid_test = None):\n",
    "    \"\"\"\n",
    "    Predicts the objects in the given image or video.\n",
    "    :param model: The model to predict the objects.\n",
    "    :param img_test: The image to predict the objects.\n",
    "    :param vid_test: The video to predict the objects.\n",
    "    :return: The image or video with the predicted objects.\n",
    "    \"\"\"\n",
    "    if img_test is not None:\n",
    "        img = Image.open(img_test)\n",
    "        img = model(img)\n",
    "        img.show()\n",
    "    if vid_test is not None:\n",
    "        vid = cv2.VideoCapture(vid_test)\n",
    "        frame_width = int(vid.get(3))\n",
    "        frame_height = int(vid.get(4))\n",
    "        size = (frame_width, frame_height)\n",
    "        result = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 10, size)\n",
    "        while True:\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            img = Image.fromarray(frame)\n",
    "            img = model(img)\n",
    "            result.write(np.array(img))\n",
    "        vid.release()\n",
    "        result.release()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(data, epochs, time, patience, batch, imgsz, save=True, device=None, worker=-1, project=None, name=None, pretrained=True, verbose=False, seed=0, optimizer='auto')\n",
    "    val(data, device = None, worker = -1, verbose = False, seed = 0)\n",
    "    predict(img_test = None, vid_test = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Structure run\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='YOLOv8 Training and Inference')\n",
    "\n",
    "    # Add arguments for training\n",
    "    parser.add_argument('--train', action = 'store_true', help = 'Train the model')\n",
    "    parser.add_argument('--data', type = str, default = 'path/to/data.yaml', help = 'Path to the data YAML file')\n",
    "    parser.add_argument('--epochs', type = int, default = 100, help = 'Number of epochs for training')\n",
    "    parser.add_argument('--time', type = float, default = 3600, help = 'Time limit for training (in seconds)')\n",
    "    parser.add_argument('--patience', type = int, default = 100, help = 'Patience for early stopping')\n",
    "    parser.add_argument('--batch', type = int, default = 16, help = 'Batch size for training')\n",
    "    parser.add_argument('--imgsz', type = int, default = 640, help = 'Image size for training')\n",
    "    parser.add_argument('--save', action = 'store_true', help = 'Save the trained model')\n",
    "    parser.add_argument('--device', type = str, default = None, help = 'Device for training (cpu or cuda)')\n",
    "    parser.add_argument('--worker', type = int, default = -1, help = 'Number of workers for data loading')\n",
    "    parser.add_argument('--project', type = str, default = 'runs/train', help = 'Project directory for saving the model')\n",
    "    parser.add_argument('--name', type = str, default = 'exp', help = 'Name of the experiment')\n",
    "    parser.add_argument('--pretrained', action = 'store_true', help = 'Use a pretrained model')\n",
    "    parser.add_argument('--verbose', action = 'store_true', help = 'Print training information')\n",
    "    parser.add_argument('--seed', type = int, default = None, help = 'Random seed for training')\n",
    "    parser.add_argument('--optimizer', type = str, default = 'auto', help = 'Optimizer for training')\n",
    "\n",
    "    # Add arguments for validation\n",
    "    parser.add_argument('--val', action = 'store_true', help = 'Validate the model')\n",
    "    parser.add_argument('--val_data', type = str, default = 'path/to/val_data.yaml', help = 'Path to the validation data YAML file')\n",
    "\n",
    "    # Add arguments for inference\n",
    "    parser.add_argument('--predict', action = 'store_true', help = 'Perform inference on an image or video')\n",
    "    parser.add_argument('--img', type = str, default = None, help = 'Path to the input image')\n",
    "    parser.add_argument('--vid', type = str, default = None, help = 'Path to the input video')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Train the model\n",
    "    if args.train:\n",
    "        train(model, args.data, args.epochs, args.time, args.patience, args.batch, args.imgsz, args.save, args.device, args.worker, args.project, args.name, args.pretrained, args.verbose, args.seed, args.optimizer)\n",
    "\n",
    "    # Validate the model\n",
    "    elif args.val:\n",
    "        val(model, args.val_data)\n",
    "\n",
    "    # Perform inference\n",
    "    elif args.predict:\n",
    "        if args.img:\n",
    "            predict(model, img=args.img)\n",
    "        elif args.vid:\n",
    "            predict(model, vid=args.vid)\n",
    "        else:\n",
    "            print(\"Please provide either an image or a video for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
